\section{Limitations and Dual-Use Considerations}
\paragraph{Limitations.} The system detects manipulation \textit{markers}, not intent, and is sensitive to context (e.g., roleplay or satire). Training data is predominantly English and derived from GPT-3.5/GPT-4 logs, which may not generalize to other models or cultural norms. LLM-judge labeling introduces stochasticity and model bias; although reliability tests are implemented, consistency remains imperfect. The embedding classifier lacks conversational context, which can produce false positives when manipulative language appears in quoted or critical discussion.

\paragraph{Temporal and model coverage gap.} WildChat contains conversations from 2023--2024 with GPT-3.5 and GPT-4 only. Frontier models deployed in 2025+ (Claude 3.5, GPT-4o, Gemini 1.5, o1) may exhibit different manipulation patterns due to updated RLHF and alignment techniques. New capabilities such as longer context windows, multimodal inputs, and agentic behaviors may introduce novel dark pattern categories not represented in current benchmarks. \textbf{The AI safety community needs a WildChat-equivalent dataset for frontier models}---real conversation logs with opt-in consent, regular updates to track behavioral drift, and cross-provider coverage to identify vendor-specific patterns. Our methodology is model-agnostic and can be extended given such data.

\paragraph{Dual-use risks.} The same detection pipeline could be used to optimize manipulative behaviors, craft evasion prompts, or fine-tune more persuasive models. Mitigations recommended in the repository documentation include limiting access to high-confidence labeled examples, rate-limiting APIs to reduce adversarial probing, and releasing aggregated statistics rather than a manipulation playbook.
